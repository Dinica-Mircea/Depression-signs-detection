\chapter{Model Selection and Hyperparameter Tuning: Optimizing AI for Depression Detection}

\label{chap:ch3}

\quad In the quest to develop an effective AI system for detecting depression from textual data, the choice of the right model and the fine-tuning of its parameters emerge as critical steps. This chapter delves into the intricate process of selecting Random Forest as the preferred model for our task. Known for its robustness and ability to handle complex datasets, Random Forest stands out as a powerful tool in the landscape of machine learning algorithms. However, the journey from selection to optimization is nuanced, involving a series of strategic decisions aimed at enhancing the model's performance.

\section{The Strategic Choice of Random Forest for Depression Detection}

In the ever-expanding realm of machine learning, selecting the most appropriate algorithm is paramount to the success of any predictive modeling task. This is particularly true in the domain of depression detection, where the complexity and variability of the data demand an approach that is not only accurate but also robust and interpretable. Drawing upon the findings of a comprehensive study that evaluated twelve distinct machine learning algorithms across seven datasets\cite{siraj2023performanceModelComparison}, we anchored our decision to employ Random Forest (RF) as the cornerstone of our analysis.

The study \cite{siraj2023performanceModelComparison} in question meticulously compared the performance of several algorithms, including Naive Bayes (NB), Linear Discriminant Analysis (LDA), Logistic Regression (LR), Artificial Neural Networks (ANN), Support Vector Machines (SVM), K-Nearest Neighbors (K-NN), Hoeffding Tree (HT), Decision Tree (DT), C4.5, Classification and Regression Tree (CART), Random Forest (RF), and Bayesian Belief Networks (BB), across multiple metrics. Among these, Random Forest emerged as the clear frontrunner, exhibiting superior accuracy, precision, and Matthew’s Correlation Coefficient (MCC). Following Random Forest, the algorithms of Neural Networks (NN), Naive Bayes (NB), Bayesian Belief Networks (BB), and Logistic Regression (LR) were identified as the next most effective, in descending order of accuracy.

The study \cite{siraj2023performanceModelComparison} also highlighted the significance of the kappa statistic and Root Mean Square Error (RMSE) as vital factors in assessing model performance, further validating the robustness of Random Forest in handling diverse and complex datasets. Inspired by these compelling insights, and in alignment with the study’s conclusion, our selection of Random Forest is underpinned by its demonstrated efficacy across multiple evaluative dimensions.

The datasets utilized for the comparative study are varied, each with its unique characteristics and relevance to different classification tasks:
\begin{itemize}
 
\item Breast Cancer Wisconsin (Original): This dataset contains 11 attributes and is used for binary classification (two classes) with 699 instances. It does include missing values, which would require additional preprocessing steps.

\item Statlog (Vehicle Silhouettes): Comprising 19 attributes over 846 instances, this dataset is for multiclass classification with four distinct classes and has no missing values.

\item Vertebral Column: With 7 attributes and 310 instances, this dataset is also used for multiclass classification, distinguishing among three classes, without any missing values.

\item Breast Tissue: This dataset has 10 attributes across 106 instances and is used for a more complex multiclass classification task with six classes, also free of missing values.

\item Contraceptive Method Choice: It includes 10 attributes and a larger number of instances at 1473. It’s structured for multiclass classification into three classes, and there are no missing values.

\item Image Segmentation: This is a sizable dataset with 20 attributes and 2310 instances for multiclass classification involving seven classes, and it contains no missing values.

\item Artificial Characters: The largest among the datasets listed, it boasts 8 attributes across a substantial 10218 instances. It’s designed for a multiclass classification with ten classes, and like most others here, it lacks missing values.

\end{itemize}

In the context of our study focused on depression detection, our model resembles the Breast Cancer Wisconsin dataset, because we are also tackling a binary classification problem. However, our model differentiates itself with a higher dimensionality, processing 64 input attributes, which poses a greater complexity in feature representation and selection. (Random Forest) RF achieved the highest accuracy at 97.85\%, suggesting it was the most successful in correctly identifying cases of breast cancer. It also topped the charts with the highest kappa value of 95.03\%, indicating a strong agreement between the predictions and the actual classifications. Precision with RF was outstanding as well, hitting a high of 98\%, while its recall was nearly as impressive at 97.9\%, underscoring its ability to identify most of the positive cases. 

Across the rest of the datasets analyzed in the study \cite{siraj2023performanceModelComparison}, Random Forest (RF) consistently delivered standout performance. Its F-measure and Matthew's Correlation Coefficient (MCC) values were notably high, often outperforming other algorithms. For instance, RF attained an accuracy of 98.48\%, kappa value of 98.23\%, and precision and recall rates both at 98.5\% on certain datasets, alongside an exceptional specificity of up to 99.7%.

While K-NN and Logistic Regression (LR) also demonstrated strong performances in certain cases, with K-NN leading in precision and recall in the Breast Tissue dataset and LR excelling with the highest MCC values for the Vehicle and Vertebral Column datasets, RF's overall dominance was clear. RF's ability to achieve the lowest error rates, coupled with the lowest root mean square error in the majority of datasets, further confirms its robustness and reliability as an algorithm for complex predictive tasks, including depression detection.

In summary, the numerical evidence from the study \cite{siraj2023performanceModelComparison} underlines RF's superior ability to handle complex predictive tasks, making it the algorithm of choice for our model aimed at accurately detecting depressive patterns within textual data.


